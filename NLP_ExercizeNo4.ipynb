{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_ExercizeNo4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2NX1iKhJg3p",
        "outputId": "7a75a3be-1111-4640-c9f0-394acfdacf5e"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tje3BPyKOCIO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a3117d1-20ee-414e-caca-d38d8f52beb9"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "# Get nltk stopword list into a set.\n",
        "stop_words = set(stopwords.words('english'))\n",
        " \n",
        "# Open and read in a text file.\n",
        "txt_file = open(\"/content/drive/MyDrive/data_in.txt\")\n",
        "txt_line = txt_file.read()\n",
        "txt_words = txt_line.split()\n",
        " \n",
        "# stopwords found counter.\n",
        "sw_found = 0\n",
        " \n",
        "# If each word checked is not in stopwords list,\n",
        "# then append word to a new text file.\n",
        "for check_word in txt_words:\n",
        "    if not check_word.lower() in stop_words:\n",
        "        # Not found on stopword list, so append.\n",
        "        appendFile = open('/content/drive/MyDrive/stopwords-removed.txt','a')\n",
        "        appendFile.write(\" \"+check_word)\n",
        "        appendFile.close()\n",
        "    else:\n",
        "        # It's on the stopword list\n",
        "        sw_found +=1\n",
        "        print(check_word)\n",
        " \n",
        "print(sw_found,\"stop words found and removed\")\n",
        "print(\"Saved as 'stopwords-removed.txt' \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "how\n",
            "are\n",
            "is\n",
            "Its\n",
            "here\n",
            "how\n",
            "are\n",
            "is\n",
            "Its\n",
            "here\n",
            "how\n",
            "are\n",
            "is\n",
            "Its\n",
            "here\n",
            "is\n",
            "It\n",
            "is\n",
            "to\n",
            "this\n",
            "is\n",
            "in\n",
            "this\n",
            "It\n",
            "be\n",
            "for\n",
            "of\n",
            "We\n",
            "to\n",
            "of\n",
            "30 stop words found and removed\n",
            "Saved as 'stopwords-removed.txt' \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtHW5y7QWGrZ",
        "outputId": "ba3de23d-844f-421e-9421-1fac6a13de67"
      },
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "0 stop words found and removed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLmovSqXrrhB",
        "outputId": "9ef9abca-4d98-4973-b7ac-6db579485ae6"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "ps = SnowballStemmer('english')\n",
        "\n",
        "txt_file = open(\"/content/drive/MyDrive/stopwords-removed.txt\")\n",
        "txt_line = txt_file.read()\n",
        "txt_words = nltk.word_tokenize(txt_line)\n",
        " \n",
        "punctuations=\"?:!.,;\"\n",
        "\n",
        " \n",
        "# If each word checked is not in stopwords list,\n",
        "# then append word to a new text file.\n",
        "for check_word in txt_words:\n",
        "     if check_word in punctuations :\n",
        "        txt_words.remove(check_word)\n",
        "\n",
        "     else :\n",
        "        # Not found on stopword list, so append.\n",
        "              appendFile = open('/content/drive/MyDrive/Stemming.txt','a')\n",
        "              appendFile.write(\" \"+ps.stem(check_word))\n",
        "              appendFile.close()\n",
        "\n",
        "\n",
        "print(\"Saved as 'Stemming.txt' \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved as 'Stemming.txt' \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi0nS_V_p2G3",
        "outputId": "73fd503a-0ef2-407e-85cc-e0d94a583ef6"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HbGGEKdv4sA",
        "outputId": "03c82894-8d4d-4868-eba8-dd756db41478"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wn_lemat = WordNetLemmatizer()\n",
        "import nltk\n",
        "\n",
        "txt_file = open(\"/content/drive/MyDrive/stopwords-removed.txt\")\n",
        "txt_line = txt_file.read()\n",
        "txt_words = nltk.word_tokenize(txt_line)\n",
        " \n",
        "punctuations=\"?:!.,;\"\n",
        "\n",
        " \n",
        "# If each word checked is not in stopwords list,\n",
        "# then append word to a new text file.\n",
        "for check_word in txt_words:\n",
        "     if check_word in punctuations :\n",
        "        txt_words.remove(check_word)\n",
        "\n",
        "     else :\n",
        "        # Not found on stopword list, so append.\n",
        "              appendFile = open('/content/drive/MyDrive/lemmatization.txt','a')\n",
        "              appendFile.write(\" \"+wn_lemat.lemmatize(check_word))\n",
        "              appendFile.close()\n",
        "\n",
        "\n",
        "print(\"Saved as 'lemmatization.txt' \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved as 'lemmatization.txt' \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}